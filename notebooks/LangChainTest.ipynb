{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd97d22d-ddb6-4518-b7bd-be69c2ba78c2",
   "metadata": {},
   "source": [
    "# This is a basic test from LangChain framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759dc2b-0907-4225-8212-f1bbf3bb2459",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa25aa9-59e2-454b-8969-bbf522b27443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4e019-e897-4ebd-95b2-d889573b0fe5",
   "metadata": {},
   "source": [
    "## Implementing the Retrieval-augmented generation ([RAG](https://python.langchain.com/docs/use_cases/question_answering/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930031b0-5201-484d-8402-c69ca74507f6",
   "metadata": {},
   "source": [
    "![Retrieval-augmented generation](RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666796c-9ce5-4db8-9dd5-37bd57c48f07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5c5d2-7753-40ce-9288-3fbf82496b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import WhatsAppChatLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf88064-1457-4fdd-abb6-bb3d641f3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = \"/Users/bernal/Documents/ext/GitRepos\"\n",
    "#loader = DirectoryLoader(directory, glob=\"**/README.md\")\n",
    "path = \"/Users/bernal/Documents/ext/GitRepos/generative-ai-knowledge/whatsapp/McFlanagan.txt\"\n",
    "loader = WhatsAppChatLoader(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60575d2a-b765-4cf1-baf7-caf3fc59f07c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Splitting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149244df-8dd1-468e-8f56-7093083ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "splits = text_splitter.split_documents(loader.load())\n",
    "#splits = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db54a5b-2abe-4cb2-9031-be50bb66c6a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Storage (Embed and store splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441be1c-9cc0-4985-afa9-226ec011e78a",
   "metadata": {},
   "source": [
    "#### Embeddings cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10159235-5948-4b43-bff9-8cbe48686169",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Embeddings: OpenAI case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67100465-49a5-41d1-9b78-182d560ad5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "emb = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a110f-1ce3-4388-9876-34a3326f1578",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Embeddings: HuggingFaceBgeEmbeddings case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164d64a-d53b-4f09-9441-1e19212b5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "#emb = OpenAIEmbeddings()\n",
    "#emb = GPT4AllEmbeddings()\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "emb = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dfe4da-b22b-4098-b425-ec7452cbb282",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Embeddings: GPT4All case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74593b-78d2-46d4-93b9-4fadc8d8a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "emb = GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd440cae-df65-4a47-b991-46325c01509a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Embeddings Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f835ecf-b00d-43cd-a9be-e44f9bd7934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits,embedding=emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce3852-361f-4605-afe6-2658639f1d4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Retrieve fast test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554f07b-db77-4ae2-92f6-24494b8799af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"bla bla bla\"\n",
    "#docs = vectorstore.similarity_search_with_relevance_scores(question, k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ce74c-df41-4db2-8171-61ccc7f1ed76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab220b-c76f-471a-b1bd-3ef8d4827dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8c840-8550-4172-b051-608782d3eea1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### retrieve fast check from retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5368974-9539-4934-a9d7-635c82a22b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"bla bla bla?\"\n",
    "#docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09f6a1-0f54-4bda-a24a-c71e4954ddcb",
   "metadata": {},
   "source": [
    "### Output (Generate response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f58e8-b5bc-4c27-b97e-069b4aa03312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### LLM: OpenAI model case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c4938-2eda-4341-b496-48e590084b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "lmm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5216c3-b7c6-4922-9fdc-68060d2e2fcd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### LLM: LLamaCpp case (Local Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a0420-81ea-4b5e-aaaa-ea8dd55e5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-cpp-python\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "def get_llm():\n",
    "    #return ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    n_gpu_layers = 0  # Metal set to 1 is enough.\n",
    "    n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    \n",
    "    # Make sure the model path is correct for your system!\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"/Users/bernal/Documents/ext/GitRepos/genrative-ai-knowledge/notebooks/llama2/llama2-13b-tiefighter.Q3_K_S.gguf\",\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_batch=n_batch,\n",
    "        n_ctx=4096,\n",
    "        f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b409e33-440c-494d-b719-4574faa7345c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### LMM: chat4all (local execution. No GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a55490-d2c3-48b6-8633-e219da37a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt4all\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "def get_llm():\n",
    "    local_path = (\n",
    "        \"/Users/bernal/Documents/ext/GitRepos/generative-ai-knowledge/notebooks/gpt4all/mistral-7b-openorca.Q4_0.gguf\"  # replace with your desired local file path\n",
    "    )\n",
    "\n",
    "    # Callbacks support token-wise streaming\n",
    "    callbacks = [StreamingStdOutCallbackHandler()]\n",
    "    \n",
    "    # Verbose is required to pass to the callback manager\n",
    "    llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n",
    "    return llm\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ec565-8e6a-4b0b-9f3f-f15b0a167d38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Simple LMM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abda0c-9324-4995-959d-637d736dbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "#question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "question = \"Tell in just a sentence what 'active metadata' is in the context of data governance\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1e2b1-7cab-477d-ba5c-49af11d2c14c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generating execution in chat mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83c815-e600-4c6c-b4b4-206750f6006e",
   "metadata": {},
   "source": [
    "We will use a conversational agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148f096-0cab-4084-8aab-508a9922dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tools\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "retrieval_tool = create_retriever_tool(\n",
    "    retriever, \n",
    "    \"search-readme-files\",\n",
    "    \"Searches and returns documents regarding the readme-files.\"\n",
    ")\n",
    "# ... more tools ...\n",
    "tools = [retrieval_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24733e31-945f-45d8-ad18-e64b96bbf381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message for the chat\n",
    "from langchain.schema.messages import SystemMessage\n",
    "\n",
    "system_message = SystemMessage(\n",
    "    content=(\n",
    "        \"Do your best to answer the questions. \"\n",
    "        \"Feel free to use any tools available to look up \"\n",
    "        \"relevant information, only if necessary\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c68d0-c0c4-4298-8fed-69639afff6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set memory key variable\n",
    "memory_key='chat_history'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9a2d2-f63c-43f8-8621-90e7012fde5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Build the agent. Simple way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48428888-cc99-4781-aae9-ed19b6dc690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "\n",
    "agent_executor = create_conversational_retrieval_agent(llm, tools, memory_key=memory_key, system_message=system_message, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c63a78-f338-412d-bea0-b7ee7d4f3788",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### Build the agent. Detailed way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1ef4d-e72b-4d9c-943e-a9b28d9e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "memory = AgentTokenBufferMemory(memory_key=memory_key, llm=llm, max_token_limit=2000)\n",
    "\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)],\n",
    ")\n",
    "\n",
    "agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72416168-d412-4e7a-80c3-d6e63cd362b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Execute chat input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3d71f-28f2-40a3-8d58-825630ba7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat line\n",
    "result = agent_executor({\"input\":\"Hi, I am Jorge\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7ed05-a7bb-4312-85c4-1f85b4a92863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
