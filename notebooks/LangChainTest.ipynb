{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd97d22d-ddb6-4518-b7bd-be69c2ba78c2",
   "metadata": {},
   "source": [
    "# This is a basic test from LangChain framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759dc2b-0907-4225-8212-f1bbf3bb2459",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa25aa9-59e2-454b-8969-bbf522b27443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4e019-e897-4ebd-95b2-d889573b0fe5",
   "metadata": {},
   "source": [
    "## Implementing the Retrieval-augmented generation ([RAG](https://python.langchain.com/docs/use_cases/question_answering/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930031b0-5201-484d-8402-c69ca74507f6",
   "metadata": {},
   "source": [
    "![Retrieval-augmented generation](RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666796c-9ce5-4db8-9dd5-37bd57c48f07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5c5d2-7753-40ce-9288-3fbf82496b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf88064-1457-4fdd-abb6-bb3d641f3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/Users/bernal/Documents/ext/GitRepos\"\n",
    "loader = DirectoryLoader(directory, glob=\"**/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60575d2a-b765-4cf1-baf7-caf3fc59f07c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Splitting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149244df-8dd1-468e-8f56-7093083ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "#splits = text_splitter.split_documents(loader.load())\n",
    "splits = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db54a5b-2abe-4cb2-9031-be50bb66c6a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Storage (Embed and store splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f835ecf-b00d-43cd-a9be-e44f9bd7934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "# gpt4all\n",
    "#from langchain.embeddings import GPT4AllEmbeddings\n",
    "# sentence-transformers\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "#emb = OpenAIEmbeddings()\n",
    "#emb = GPT4AllEmbeddings()\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "emb = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,embedding=emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce3852-361f-4605-afe6-2658639f1d4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Retrieve fast test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554f07b-db77-4ae2-92f6-24494b8799af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"What is active metadata?\"\n",
    "#docs = vectorstore.similarity_search_with_relevance_scores(question, k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ce74c-df41-4db2-8171-61ccc7f1ed76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab220b-c76f-471a-b1bd-3ef8d4827dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8c840-8550-4172-b051-608782d3eea1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### retrieve fast check from retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5368974-9539-4934-a9d7-635c82a22b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"What is active metadata?\"\n",
    "#docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09f6a1-0f54-4bda-a24a-c71e4954ddcb",
   "metadata": {},
   "source": [
    "### Output (Generate response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f58e8-b5bc-4c27-b97e-069b4aa03312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### LLM: OpenAI model case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c4938-2eda-4341-b496-48e590084b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "lmm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5216c3-b7c6-4922-9fdc-68060d2e2fcd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### LLM: LLamaCpp case (Local Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a0420-81ea-4b5e-aaaa-ea8dd55e5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-cpp-python\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "def get_llm():\n",
    "    #return ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    n_gpu_layers = 0  # Metal set to 1 is enough.\n",
    "    n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    \n",
    "    # Make sure the model path is correct for your system!\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"/Users/bernal/Documents/ext/GitRepos/genrative-ai-knowledge/notebooks/llama2/llama2-13b-tiefighter.Q3_K_S.gguf\",\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_batch=n_batch,\n",
    "        n_ctx=4096,\n",
    "        f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b409e33-440c-494d-b719-4574faa7345c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### LMM: chat4all (local execution. No GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a55490-d2c3-48b6-8633-e219da37a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt4all\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "def get_llm():\n",
    "    local_path = (\n",
    "        \"/Users/bernal/Documents/ext/GitRepos/generative-ai-knowledge/notebooks/gpt4all/mistral-7b-openorca.Q4_0.gguf\"  # replace with your desired local file path\n",
    "    )\n",
    "\n",
    "    # Callbacks support token-wise streaming\n",
    "    callbacks = [StreamingStdOutCallbackHandler()]\n",
    "    \n",
    "    # Verbose is required to pass to the callback manager\n",
    "    llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n",
    "    return llm\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ec565-8e6a-4b0b-9f3f-f15b0a167d38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Simple LMM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abda0c-9324-4995-959d-637d736dbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1e2b1-7cab-477d-ba5c-49af11d2c14c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generation execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3d71f-28f2-40a3-8d58-825630ba7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt_custom = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | rag_prompt_custom \n",
    "    | llm \n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is active metadata?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb7ed05-a7bb-4312-85c4-1f85b4a92863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
